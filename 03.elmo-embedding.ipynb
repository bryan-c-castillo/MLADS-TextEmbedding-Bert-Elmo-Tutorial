{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMo\n",
    "\n",
    "Retrieves sentence embedding values for movie reviews from a move review dataset using a pre-trained ELMo model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0606 20:38:49.726242 140699030759168 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "tf.logging.set_verbosity(tf.logging.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load IMDB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data = pd.read_pickle(\"{}/imdb_data.pickle.gz\".format(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_set</th>\n",
       "      <th>polarity</th>\n",
       "      <th>sentence</th>\n",
       "      <th>movie_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45161</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>I recently purchassed the very underrated Dreamcast and went off into town to find some games to use them on. I bought Soul Calibur (A classic) and then i stepped across the domain of Resident Evi...</td>\n",
       "      <td>tt0217000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37450</th>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>Exceedingly complicated and drab. I'm a bright guy, but this was just too much for a tired brain. It would really benefit from a few early clues as to who these people are and what they are doing....</td>\n",
       "      <td>tt0391479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      data_set  polarity  \\\n",
       "45161    train         1   \n",
       "37450    train         0   \n",
       "\n",
       "                                                                                                                                                                                                      sentence  \\\n",
       "45161  I recently purchassed the very underrated Dreamcast and went off into town to find some games to use them on. I bought Soul Calibur (A classic) and then i stepped across the domain of Resident Evi...   \n",
       "37450  Exceedingly complicated and drab. I'm a bright guy, but this was just too much for a tired brain. It would really benefit from a few early clues as to who these people are and what they are doing....   \n",
       "\n",
       "        movie_id  \n",
       "45161  tt0217000  \n",
       "37450  tt0391479  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving the text embedding for all samples may take a long time.\n",
    "For demo purposes it is useful to limit the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data = imdb_data.sample(800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Text\n",
    "\n",
    "Before retrieving the text embeddings the reviews are cleaned up and limited in length to speed up processing.\n",
    "\n",
    "This code is a little more complex to handle running on Windows where spacy doesn't seem to work for downloading the en corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 128\n",
    "\n",
    "text_http_re  = re.compile(r'http\\S+')\n",
    "text_digit_re = re.compile(r'[0-9]')\n",
    "text_html_re  = re.compile(r'<[^>]{0,20}>')\n",
    "text_punc_re  = re.compile('[' + re.escape('\\'!\"#$%&()*+-/:;<=>?@[\\\\]^_`{|}~') + ']')\n",
    "text_ws_re    = re.compile('\\s+')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = text_http_re.sub('', text)\n",
    "    text = text_html_re.sub('', text)\n",
    "    text = text_digit_re.sub(' ', text)\n",
    "    text = text_punc_re.sub('', text)\n",
    "    text = text_ws_re.sub(' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def create_lemmatizer_spacy():\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    def lemmatize(text):\n",
    "        return ' '.join([token.lemma_ for token in nlp(text)][0:max_words])\n",
    "    \n",
    "    return lemmatize\n",
    "\n",
    "def create_lemmatizer_nltk():\n",
    "    from nltk.stem import WordNetLemmatizer \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def lemmatize(text):\n",
    "        return ' '.join([lemmatizer.lemmatize(w) for w in text.split()][0:max_words])\n",
    "    \n",
    "    return lemmatize\n",
    "\n",
    "# Setup a lemmatize function, spacy.load may fail on windows for en.\n",
    "try:\n",
    "    lemmatize = create_lemmatizer_spacy()\n",
    "except:\n",
    "    print(\"Using nltk for lemmatization.\")\n",
    "    lemmatize = create_lemmatizer_nltk()\n",
    "            \n",
    "def process_text(text):\n",
    "    return lemmatize(clean_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data['clean_review'] = imdb_data.sentence.apply(process_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract ELMo Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0606 20:39:04.156948 140699030759168 deprecation.py:323] From /anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Utility function to break sentences into batches.\n",
    "# Passing in a single large batch can run into memory issues, but passing 1 at a time can slow down the overall process.\n",
    "def batches(sentences, batch_size):\n",
    "    results = []\n",
    "    chunk = []\n",
    "    for s in sentences:\n",
    "        chunk.append(s)\n",
    "        if len(chunk) >= batch_size:\n",
    "            yield(chunk)\n",
    "            chunk = []\n",
    "    if len(chunk) > 0:\n",
    "        yield(chunk)\n",
    "\n",
    "def elmo_create_embedding_extractor(module, batch_size=20):\n",
    "    with tf.Graph().as_default():\n",
    "        sentences = tf.placeholder(tf.string)\n",
    "        embed = hub.Module(module, trainable=True)\n",
    "        embeddings = embed(sentences)\n",
    "        session = tf.train.MonitoredSession()\n",
    "        \n",
    "    def extract(_sentences):\n",
    "        results = []\n",
    "        for s_batch in batches(_sentences, batch_size):\n",
    "            results.extend(session.run(embeddings, { sentences: s_batch }))\n",
    "            print(\"[{}] Extracted {}\".format(datetime.now(), len(results)))\n",
    "        return results\n",
    "\n",
    "    return extract\n",
    "\n",
    "elmo_get_embedding = elmo_create_embedding_extractor(\"https://tfhub.dev/google/elmo/2\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-06-06 20:39:16.807155] Extracted 100\n",
      "[2019-06-06 20:39:27.454361] Extracted 200\n",
      "[2019-06-06 20:39:38.077340] Extracted 300\n",
      "[2019-06-06 20:39:48.647735] Extracted 400\n",
      "[2019-06-06 20:39:59.235525] Extracted 500\n",
      "[2019-06-06 20:40:09.847631] Extracted 600\n",
      "[2019-06-06 20:40:20.429508] Extracted 700\n",
      "[2019-06-06 20:40:31.021744] Extracted 800\n",
      "Extraction took time  0:01:25.337014\n"
     ]
    }
   ],
   "source": [
    "current_time = datetime.now()\n",
    "imdb_data['embedding'] = elmo_get_embedding(imdb_data.clean_review.values)\n",
    "print(\"Extraction took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save embedding values to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data.to_pickle(\"{}/imdb_data_w_elmo_embedding.pickle.gz\".format(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
